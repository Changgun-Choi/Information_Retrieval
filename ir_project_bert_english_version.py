# -*- coding: utf-8 -*-
"""IR Project BERT English_version.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PH2p1SOHw6-_B9Qj6QjwXP-C5YP0uQIj

Bert-Base-Uncased
"""

!pip install transformers

from google.colab import drive
drive.mount('/content/drive')

import tensorflow as tf
from transformers import BertTokenizer, TFBertModel
from tqdm import tqdm
import os
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

tokenizer('He is rich')

!wget https://msmarco.blob.core.windows.net/msmarcoranking/triples.train.small.tar.gz
!tar -xf triples.train.small.tar.gz

##
TPU = True
if TPU:
  resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])
  tf.config.experimental_connect_to_cluster(resolver)
  tf.tpu.experimental.initialize_tpu_system(resolver)
else:
  pass

def convert_data(data_df):    # Input: tokens(tokens encoded), segments, masks
  global tokenizer
  tokens, segments, masks, targets = [], [], [] ,[]
    
  for i in range(len(data_df)):
   
    targets.append(data_df["relevant"][i])

    que = tokenizer.encode(data_df["query"][i])
    doc = tokenizer.encode(data_df["passage"][i])
        
    doc.pop(0) 
        
        # Maximum Seq_len : cutting the length
    if len(que+doc) > SEQ_LEN:  #200 
      while len(que+doc) != SEQ_LEN:
        doc.pop(-1)  # Cut context
      doc.pop(-1)
      doc.append(102) #[SEP]
          
        # Segment
        # query, passage, padding
        # 00000000, 1111111, 0000000
    segment = [0]*len(que) + [1]*len(doc) + [0]*(SEQ_LEN-len(que)-len(doc))
        
        # Masking, Padding
    if len(que + doc) <= SEQ_LEN:
      mask = [1]*len(que+doc) + [0]*(SEQ_LEN-len(que+doc))
    else:
      mask = [1]*len(que+doc)   #No mask
          
    if len(que + doc) <= SEQ_LEN:
      while len(que+doc) != SEQ_LEN: #  padding
        doc.append(0)  #passage  padding
                              
    ids = que + doc # padding 
        
    tokens.append(ids) # [ids], [ids], ........
    segments.append(segment)
    masks.append(mask)
        
    # make numpy array 
  tokens = np.array(tokens)
  segments = np.array(segments)
  masks = np.array(masks)
  targets = np.array(targets)

  return [tokens, masks, segments], targets

def load_data(pandas_dataframe):
  data_df = pandas_dataframe
  data_df["query"] = data_df["query"].astype(str)     # String
  data_df["passage"] = data_df["passage"].astype(str)
  data_x, data_y = convert_data(data_df)

  return data_x, data_y   # [tokens,masks,segments], targets

import pandas as pd 
train_pos = pd.read_csv("/content/triples.train.small.tsv",
                    sep = "\t", nrows = 800000, header = None, skiprows = 1, usecols = [0,1])  1600000
train_neg = pd.read_csv("/content/triples.train.small.tsv",
                    sep = "\t", nrows = 800000, header = None, skiprows = 1, usecols = [0,2]) 

train_pos.columns = ["query", "passage"]
train_neg.columns = ["query", "passage"]
train_pos["relevant"] = 1 # target label
train_neg["relevant"] = 0 


#train = train_pos.append(train_neg)   # List adding
train = pd.concat([train_pos, train_neg])
train.reset_index(inplace=True, drop=True)           # Resetting Index as Concat makes duplicated sets 00 11 22 33 44 
train


#########################################
import numpy as np
SEQ_LEN = 200 
BATCH_SIZE = 20         # Small for CPU, GPU        
EPOCHS=2 
LR=1.0e-5

train_x, train_y = load_data(train)        # Load Data


#########################
!pip install tensorflow-addons
import tensorflow_addons as tfa

def get_bert_finetuning_model():
  
  model = TFBertModel.from_pretrained("bert-base-uncased")
         
  token_inputs = tf.keras.layers.Input((SEQ_LEN,), dtype=tf.int32, name='input_word_ids')
  mask_inputs = tf.keras.layers.Input((SEQ_LEN,), dtype=tf.int32, name='input_masks')
  segment_inputs = tf.keras.layers.Input((SEQ_LEN,), dtype=tf.int32, name='input_segment')

  bert_outputs = model([token_inputs, mask_inputs, segment_inputs])         # Putting input into Bert

  bert_outputs = bert_outputs[1]
  classification = tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02))(bert_outputs)  # input 으로 들어감 # 쭉 연결되어 있음 Sequential 
  BertRanking_model = tf.keras.Model([token_inputs, mask_inputs, segment_inputs], classification)
  opt = tfa.optimizers.RectifiedAdam(lr=LR, weight_decay=0.0025, warmup_proportion=0.05)
  BertRanking_model.compile(optimizer=opt, loss=tf.keras.losses.BinaryCrossentropy(), metrics = ['accuracy'])     # opt 를 변수로 둘꺼면 Global variable이 아닌 def 함수 안에서 정의 

  return BertRanking_model

##########################

path = "gdrive/My Drive/Colab Notebooks/BERT"
if TPU:
  strategy = tf.distribute.experimental.TPUStrategy(resolver)
 
  with strategy.scope():
    BertRanking_model = get_bert_finetuning_model()   # Using TPU 
  BertRanking_model.fit(train_x, train_y, epochs=EPOCHS, shuffle=True, batch_size=BATCH_SIZE, validation_split= 0.2) # Fit은 TPU 안에 정의할 필요 x
  BertRanking_model.save_weights(path+"/BERT_new.h5")

  # GPU 
else:
  BertRanking_model = get_bert_finetuning_model()
  BertRanking_model.fit(train_x, train_y, epochs=EPOCHS, shuffle=True, batch_size=BATCH_SIZE, validation_split= 0.2)
  BertRanking_model.save_weights(path+"/BERT_new.h5")

  #############################################

new_model = get_bert_finetuning_model()
new_model.load_weights(path+"/BERT_new.h5")

new_model = get_bert_finetuning_model()
new_model.load_weights(path+"/BERT_new.h5")
new_model.weights

from psutil import virtual_memory
ram_gb = virtual_memory().total / 1e9
print('Your runtime has {:.1f} gigabytes of available RAM\n'.format(ram_gb))

if ram_gb < 20:
  print('To enable a high-RAM runtime, select the Runtime > "Change runtime type"')
  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')
  print('re-execute this cell.')
else:
  print('You are using a high-RAM runtime!')

"""### **1.2 Top 1000 - Relevance Score**
### Always can start again from here
"""

import numpy as np
SEQ_LEN = 200 
BATCH_SIZE = 20         # Small for CPU, GPU        
EPOCHS=2 
LR=1.0e-5
#########################
!pip install tensorflow-addons
import tensorflow_addons as tfa

def get_bert_finetuning_model():
  
  model = TFBertModel.from_pretrained("bert-base-uncased")
         
  token_inputs = tf.keras.layers.Input((SEQ_LEN,), dtype=tf.int32, name='input_word_ids')
  mask_inputs = tf.keras.layers.Input((SEQ_LEN,), dtype=tf.int32, name='input_masks')
  segment_inputs = tf.keras.layers.Input((SEQ_LEN,), dtype=tf.int32, name='input_segment')

  bert_outputs = model([token_inputs, mask_inputs, segment_inputs])         # Putting input into Bert

  bert_outputs = bert_outputs[1]
  classification = tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02))(bert_outputs)  # input 으로 들어감 # 쭉 연결되어 있음 Sequential 
  BertRanking_model = tf.keras.Model([token_inputs, mask_inputs, segment_inputs], classification)
  opt = tfa.optimizers.RectifiedAdam(lr=LR, weight_decay=0.0025, warmup_proportion=0.05)
  BertRanking_model.compile(optimizer=opt, loss=tf.keras.losses.BinaryCrossentropy(), metrics = ['accuracy'])     # opt 를 변수로 둘꺼면 Global variable이 아닌 def 함수 안에서 정의 

  return BertRanking_model

#!unzip "/content/drive/MyDrive/Colab Notebooks/BERT/all.zip" -d "/content/"
!tar -xzvf "/content/drive/MyDrive/Colab Notebooks/BERT/queries.tar.gz" -C "/content/"
!tar -xzvf "/content/drive/MyDrive/Colab Notebooks/BERT/collection.tar.gz" -C "/content/"

import datetime
import csv
import pandas as pd 
queries = pd.read_csv("/content/queries.train.tsv", sep='\t', header = None) 
passage = pd.read_csv("/content/collection.tsv", sep='\t', header = None) 
queries = queries.to_numpy()
passage = passage.to_numpy()

pointwise_model = get_bert_finetuning_model()
pointwise_model.load_weights("/content/drive/MyDrive/Colab Notebooks/BERT/BERT_new.h5")   
print("pairwise_model loaded")

def predict_convert_data(data_df):
    global tokenizer
    tokens, masks, segments = [], [], []
    
    for i in range(len(data_df)):
        que = tokenizer.encode(data_df.iloc[i,2])
        doc = tokenizer.encode(data_df.iloc[i,3])
        
        doc.pop(0) 
        # Maximum Seq_len : cutting the length
        if len(que+doc) > SEQ_LEN:  #200 
          while len(que+doc) != SEQ_LEN:
            doc.pop(-1)  # Cut context
          doc.pop(-1)
          doc.append(102) #[SEP]
          
        # Segment
        # query, passage, padding
        # 00000000, 1111111, 0000000
        segment = [0]*len(que) + [1]*len(doc) + [0]*(SEQ_LEN-len(que)-len(doc))
        
        # Masking, Padding
        if len(que + doc) <= SEQ_LEN:
          mask = [1]*len(que+doc) + [0]*(SEQ_LEN-len(que+doc))
        else:
          mask = [1]*len(que+doc)   #No mask
          
        if len(que + doc) <= SEQ_LEN:
          while len(que+doc) != SEQ_LEN: #  padding
            doc.append(0)  #passage  padding
                              
        ids = que + doc # padding 
        
        tokens.append(ids) # [ids], [ids], ........
        segments.append(segment)
        masks.append(mask)
    tokens = np.array(tokens)
    segments = np.array(segments)
    masks = np.array(masks)
    

    return [tokens, masks, segments]

def predict_load_data(pandas_dataframe):
    data_df = pandas_dataframe.copy(deep=False)
    data_df["query"] = data_df["query"].astype(str).copy(deep=False)     # String
    data_df["passage"] = data_df["passage"].astype(str).copy(deep=False)
    data_x = predict_convert_data(data_df)
    return data_x

data = pd.read_csv("/content/sample_data/all.csv", nrows = 5,  sep="\t", header=None) 
data = data.to_numpy()
data
for row in data:
  print(row)

data = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/BERT/all_dev.csv", nrows = 5, skiprows = 0,  sep="\t", header=None)   
data
data = data.to_numpy()
unqiue_ids = np.unique(data[:, 0])
(len(unqiue_ids))


def from_data_to_array(data):
    data_ma = []
    for row in data:                    # 일치하는 query                       # Passage 
        data_ma.append([row[0], row[1], queries[queries[:, 0] == row[0], 1][0], passage[passage[:, 0] == row[1], 1][0]])
    return data_ma

data
for row in data:
  print(row[0])
print(from_data_to_array(data))

data = pd.read_csv("/content/sample_data/all.csv", nrows = 5, sep=",", header=None)   
print(data)
data = data.to_numpy()
print(data)
data = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/BERT/all_dev.csv", nrows = 5, skiprows = 0,  sep="\t", header=None) 
print(data)
data = data.to_numpy()
print(data)

#data = pd.read_csv("/content/sample_data/all.csv", nrows = 5, sep=",", header=None)   
data = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/BERT/all_dev.csv", nrows = 5, skiprows = 0,  sep="\t", header=None)   
data = data.to_numpy()

def from_data_to_array(data):
    data_ma = []
    for row in data:                    #  query                               # Passage 
        data_ma.append([row[0], row[1], queries[queries[:, 0] == row[0], 1][0], passage[passage[:, 0] == row[1], 1][0]])
    return data_ma
print(from_data_to_array(data))

def take_third(elem):
    return elem[2]

def from_data_to_array(data):
    data_ma = []
    for row in data:                    #  query                       # Passage 
        data_ma.append([row[0], row[1], queries[queries[:, 0] == row[0], 1][0], passage[passage[:, 0] == row[1], 1][0]])
    return data_ma

for id in unqiue_ids:
    a = datetime.datetime.now()
    matrix = []
    data_ma = from_data_to_array(data[data[:, 0] == id])  # matching q_id
    df = pd.DataFrame(data_ma, columns = ['q_id','p_id','query', 'passage'])
    train_data = predict_load_data(df)
    preds = pointwise_model.predict(train_data)
    #print(preds)
    
    for i in range(0, len(data_ma)):
        matrix.append([data_ma[i][0], data_ma[i][1], np.float32(preds[i])])
    matrix = sorted(matrix, key=take_third, reverse=True)
    matrix = pd.DataFrame(matrix,columns=['q_id','p_id','score'])
    #print(matrix)

    matrix.to_csv("result" + str(id) + ".csv", index = False)

! zip pointwise_results.zip *.csv
print("ZIP done!!!!!")

from google.colab import files
files.download(pointwise_results.zip)
    #with open("/content/drive/MyDrive/Colab Notebooks/BERT/result" + str(id) + ".csv", 'w', newline='') as file:
    #    mywriter = csv.writer(file, delimiter=',')
    #    mywriter.writerows(matrix)
    #print("ID: ", id, " Time: ", datetime.datetime.now() - a)

upload = files.upload(pointwise_results.zip)

ls -l









"""전에 작성한것

"""

path="top1000.dev"  

top_train = pd.read_csv(path,sep = "\t", nrows = 100, header = None)  10,
top_train.columns = ["q_id", "p_id", 'query', "passage"]

print(top_train)

#train_sent = []
#for s in top_train:
#   train_sent.append(s[1].lower())

query_list = top_train.q_id.unique()
query_list
data = []
with strategy.scope():
  for index, query_id in enumerate(query_list):
    top = top_train.loc[top_train.q_id == query_id] 
    top.reset_index(inplace=True, drop=True) # Load 하기 위해서 필요
    train_data = predict_load_data(top)
    #print(top)
    preds = BertRanking_model.predict(train_data)
      0.8 
    #top = top.to_numpy()                                                                     
    #data.append([top[index].astype('int64'), top["p_id"].astype('int64'), top["query"].astype(str), top["passage"].astype(str), preds])
    top["score"] = preds
    #print(top)
    top.sort_values(by =["score"],ascending = True)
    top.append(top)
  print(top)
  #df = pd.DataFrame(data, columns=['q_id', 'p_id ', 'query', 'passage', 'value'])
  #print(df)

top_train.to_csv (r'C:/Users/ChangGun Choi/Desktop/0. 수업자료/0. IR project/BERTexport_dataframe.csv', index = False, header=True)

print (df)

"""### 2.1 Pairwise  Duo-Bert"""

from google.colab import drive
drive.mount('/content/drive')

!pip install transformers
#!wget https://msmarco.blob.core.windows.net/msmarcoranking/triples.train.small.tar.gz

!tar -xzvf "/content/drive/MyDrive/Colab Notebooks/BERT/triples.train.small.tar.gz" -C "/content/"

import tensorflow as tf
from transformers import BertTokenizer, TFBertModel
from tqdm import tqdm
import os
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
tokenizer('He is rich','I am rich','I am poor')

################################# #500000
import pandas as pd 
trainpair_pos = pd.read_csv("/content/triples.train.small.tsv", sep = "\t", nrows = 1, header = None, skiprows = 1, usecols = [0,1,2]) 
#trainpair_neg = pd.read_csv("/content/triples.train.small.tsv", sep = "\t", nrows = 1, header = None, skiprows = 1, usecols = [0,1,2]) 
trainpair_pos.columns = ["query", "passage_rel", "passage_non"]                 # 500000
trainpair_neg = trainpair_pos
trainpair_neg.columns = ["query", "passage_non", "passage_rel"]
trainpair_pos["relevant"] = 1
trainpair_neg["relevant"] = 0

trainpair = pd.concat([trainpair_pos, trainpair_neg])
trainpair.reset_index(inplace=True, drop=True)   # Resetting Index
trainpair

import numpy as np
SEQ_LEN = 512
BATCH_SIZE = 20         # Small for CPU, GPU        
EPOCHS=20 
LR=1.0e-5

trainpair_pos

trainpair_neg

TPU = True
if TPU:
  resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])
  tf.config.experimental_connect_to_cluster(resolver)
  tf.tpu.experimental.initialize_tpu_system(resolver)
else:
  pass

def convert_data(data_df):    # Input: tokens(tokens encoded), segments, masks
  global tokenizer
  tokens, segments, masks, targets = [], [], [] ,[]
    
  for i in range(len(data_df)):    
    targets.append(data_df["relevant"][i])

    que = tokenizer.encode(data_df["query"][i])
    doc_1 = tokenizer.encode(data_df["passage_rel"][i])
    doc_2 = tokenizer.encode(data_df["passage_non"][i])
        
    doc_1.pop(0) # context의 맨 앞에 있는 [CLS]에 해당하는 101을 삭제
    doc_2.pop(0) 
        
        # Maximum Seq_len : cutting the length
    if len(que+doc_1+doc_2) > SEQ_LEN:  #512
      while len(que+doc_1+doc_2) != SEQ_LEN:
        doc_2.pop(-1)  #   최대 길이인 512가 넘지 않도록 같아질때까지 잘라줌 
      doc_2.pop(-1)
      doc_2.append(102) #[SEP]
          
    # 1) Segment
    # query, passage, padding
    # 00000 11111 222222 000000
    segment = [0]*len(que) + [1]*len(doc_1) + [2]*len(doc_2) + [0]*(SEQ_LEN-len(que)-len(doc_1)-len(doc_2))
        
    # 2) Masking
    if len(que + doc_1 + doc_2) <= SEQ_LEN:
      mask = [1]*len(que+doc_1+doc_2) + [0]*(SEQ_LEN-len(que)-len(doc_1)-len(doc_2))       
    else:
      mask = [1]*len(que+doc_1+doc_2)   #No mask

    # 3) Padding for Token Embedding      
    if len(que + doc_1 + doc_2) <= SEQ_LEN:
      while len(que + doc_1 + doc_2) != SEQ_LEN: 
        doc_2.append(0)  #passage_non padding
                              
    ids = que + doc_1 + doc_2 # padding 
        
    tokens.append(ids) # [ids], [ids], ........
    segments.append(segment)
    masks.append(mask)
        
    # make numpy array 
  tokens = np.array(tokens)
  segments = np.array(segments)
  masks = np.array(masks)
  targets = np.array(targets)

  return [tokens, masks, segments], targets

def load_data(pandas_dataframe):
  data_df = pandas_dataframe
  data_df["query"] = data_df["query"].astype(str)     # String
  data_df["passage_rel"] = data_df["passage_rel"].astype(str)
  data_df["passage_non"] = data_df["passage_non"].astype(str)
  data_x, data_y = convert_data(data_df)

  return data_x, data_y   # [tokens, masks, segments], targets  = (X. Y)


train_x, train_y = load_data(trainpair)        # Load Data

###################################################################################################

!pip install tensorflow-addons
import tensorflow_addons as tfa

def get_bert_finetuning_model():
  model = TFBertModel.from_pretrained("bert-base-uncased")
         
  token_inputs = tf.keras.layers.Input((SEQ_LEN,), dtype=tf.int32, name='input_word_ids')
  mask_inputs = tf.keras.layers.Input((SEQ_LEN,), dtype=tf.int32, name='input_masks')
  segment_inputs = tf.keras.layers.Input((SEQ_LEN,), dtype=tf.int32, name='input_segment')

  bert_outputs = model([token_inputs, mask_inputs, segment_inputs])      # Putting input into Bert
  bert_outputs = bert_outputs[1]
  bert_outputs = tf.keras.layers.Dropout(0.5)(bert_outputs)
  classification = tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02))(bert_outputs)  # input 으로 들어감 # 쭉 연결되어 있음 Sequential 
  BertRanking_model = tf.keras.Model([token_inputs, mask_inputs, segment_inputs], classification) # input, output 
  
  opt = tfa.optimizers.RectifiedAdam(learning_rate=LR, weight_decay=0.0025, warmup_proportion=0.05)
  loss = tf.keras.losses.BinaryCrossentropy()
  BertRanking_model.compile(optimizer=opt, loss=loss, metrics = ['accuracy'])     # opt 를 변수로 둘꺼면 Global variable이 아닌 def 함수 안에서 정의 

  return BertRanking_model

##########################
path = "/content/drive/MyDrive/Colab Notebooks/BERT" 
if TPU:
  strategy = tf.distribute.experimental.TPUStrategy(resolver)
 
  with strategy.scope():
    BertRanking_model = get_bert_finetuning_model()   # Using TPU 
  BertRanking_model.fit(train_x, train_y, epochs=EPOCHS, shuffle=True, batch_size=BATCH_SIZE, validation_split= 0.2) # Fit은 TPU 안에 정의할 필요 x
  BertRanking_model.save_weights(path+"/BERT_pairwise.h5")
  print("saved")
  # GPU 
else:
  BertRanking_model = get_bert_finetuning_model()
  BertRanking_model.fit(train_x, train_y, epochs=EPOCHS, shuffle=True, batch_size=BATCH_SIZE, validation_split= 0.2)
  BertRanking_model.save_weights(path+"/BERT_pairwise.h5")
  print("saved")
  #############################################

train_x

path = "/content/drive/MyDrive/Colab Notebooks/BERT" 
 BertRanking_model.save_weights(path+"/BERT_pairwise.h5")

train = train_x, train_y
#with open("/content/drive/MyDrive/Colab Notebooks/BERT/result" + str(id) + ".csv", 'w', newline='') as file:
    #    mywriter = csv.writer(file, delimiter=',')
    #    mywriter.writerows(matrix)







train = np.array(train_x, train_y)
np.savetxt(fname="train.csv", delimiter=",", X= train)
train_csv = np.loadtxt(fname="train.csv", delimiter=",")
print(train_csv)

#with open("/content/drive/MyDrive/Colab Notebooks/BERT/result" + str(id) + ".csv", 'w', newline='') as file:
    #    mywriter = csv.writer(file, delimiter=',')
    #    mywriter.writerows(matrix)

!pip install tensorflow-addons
import tensorflow_addons as tfa

def get_bert_finetuning_model():
  model = TFBertModel.from_pretrained("bert-base-uncased")
         
  token_inputs = tf.keras.layers.Input((SEQ_LEN,), dtype=tf.int32, name='input_word_ids')
  mask_inputs = tf.keras.layers.Input((SEQ_LEN,), dtype=tf.int32, name='input_masks')
  segment_inputs = tf.keras.layers.Input((SEQ_LEN,), dtype=tf.int32, name='input_segment')

  bert_outputs = model([token_inputs, mask_inputs, segment_inputs])      # Putting input into Bert
  bert_outputs = bert_outputs[1]
  bert_outputs = tf.keras.layers.Dropout(0.5)(bert_outputs)
  classification = tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02))(bert_outputs)  # input 으로 들어감 # 쭉 연결되어 있음 Sequential 
  BertRanking_model = tf.keras.Model([token_inputs, mask_inputs, segment_inputs], classification) # input, output 
  
  opt = tfa.optimizers.RectifiedAdam(lr=LR, weight_decay=0.0025, warmup_proportion=0.05)
  loss = tf.keras.losses.BinaryCrossentropy()
  BertRanking_model.compile(optimizer=opt, loss=loss, metrics = ['accuracy'])     # opt 를 변수로 둘꺼면 Global variable이 아닌 def 함수 안에서 정의 

  return BertRanking_model

##########################
path = "gdrive/My Drive/Colab Notebooks/BERT"
if TPU:
  strategy = tf.distribute.experimental.TPUStrategy(resolver)
 
  with strategy.scope():
    BertRanking_model = get_bert_finetuning_model()   # Using TPU 
  BertRanking_model.fit(train_x, train_y, epochs=EPOCHS, shuffle=True, batch_size=BATCH_SIZE, validation_split= 0.2) # Fit은 TPU 안에 정의할 필요 x
  BertRanking_model.save_weights(path+"/BERT_pairwise.h5")
  print("saved")
  # GPU 
else:
  BertRanking_model = get_bert_finetuning_model()
  BertRanking_model.fit(train_x, train_y, epochs=EPOCHS, shuffle=True, batch_size=BATCH_SIZE, validation_split= 0.2)
  BertRanking_model.save_weights(path+"/BERT_pairwise.h5")
  print("saved")
  #############################################







"""## Top 50 after Pointwise ///  Pairwise prediction score  50 X 49   **bold text**"""

pairwise_model = get_bert_finetuning_model()
pairwise_model.load_weights("/content/drive/MyDrive/Colab Notebooks/BERT/BERT_pairwise.h5")   
print("pairwise_model loaded")

import numpy as np
SEQ_LEN = 200 
BATCH_SIZE = 20         # Small for CPU, GPU        
EPOCHS=2 
LR=1.0e-5
#########################
!pip install tensorflow-addons
import tensorflow_addons as tfa

def get_bert_finetuning_model():
  
  model = TFBertModel.from_pretrained("bert-base-uncased")
         
  token_inputs = tf.keras.layers.Input((SEQ_LEN,), dtype=tf.int32, name='input_word_ids')
  mask_inputs = tf.keras.layers.Input((SEQ_LEN,), dtype=tf.int32, name='input_masks')
  segment_inputs = tf.keras.layers.Input((SEQ_LEN,), dtype=tf.int32, name='input_segment')

  bert_outputs = model([token_inputs, mask_inputs, segment_inputs])         # Putting input into Bert

  bert_outputs = bert_outputs[1]
  classification = tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02))(bert_outputs)  # input 으로 들어감 # 쭉 연결되어 있음 Sequential 
  BertRanking_model = tf.keras.Model([token_inputs, mask_inputs, segment_inputs], classification)
  opt = tfa.optimizers.RectifiedAdam(lr=LR, weight_decay=0.0025, warmup_proportion=0.05)
  BertRanking_model.compile(optimizer=opt, loss=tf.keras.losses.BinaryCrossentropy(), metrics = ['accuracy'])     # opt 를 변수로 둘꺼면 Global variable이 아닌 def 함수 안에서 정의 

  return BertRanking_model











!unzip "/content/drive/MyDrive/Colab Notebooks/BERT/all.zip" -d "/content/"
!tar -xzvf "/content/drive/MyDrive/Colab Notebooks/BERT/queries.tar.gz" -C "/content/"
!tar -xzvf "/content/drive/MyDrive/Colab Notebooks/BERT/collection.tar.gz" -C "/content/"

import datetime
import csv
import pandas as pd 
queries = pd.read_csv("/content/queries.train.tsv", sep='\t', header = None) 
passage = pd.read_csv("/content/collection.tsv", sep='\t', header = None) 
queries = queries.to_numpy()
passage = passage.to_numpy()

data = pd.read_csv("C:/Users/ChangGun Choi/Desktop/0. 수업자료/0. IR project/File/all.csv", nrows = 1005, sep=",", header=None)   
data = data.to_numpy()
unqiue_ids = np.unique(data[:, 0])

pairwise_model = get_bert_finetuning_model()
pairwise_model.load_weights(path+"/BERT_pairwise.h5")
print("pairwise_model loaded")

all_dev





"""**모델의 FLOW를 확인해 보도록 하겠습니다.**

훈련을 시작합니다.  
1epoch을 훈련해 보고, 결과를 확인하고 훈련을 다시 시작할 예정입니다.
"""

!nvidia-smi

sess = K.get_session()
uninitialized_variables = set([i.decode('ascii') for i in sess.run(tf.report_uninitialized_variables())])
init = tf.variables_initializer([v for v in tf.global_variables() if v.name.split(':')[0] in uninitialized_variables])
sess.run(init)

bert_model = get_bert_finetuning_model(model)
bert_model.summary()
# 훈련 시작
history = bert_model.fit(train_x, train_y, validation_split=0.1, batch_size=12, shuffle=True, verbose=1, epochs=2)

from sklearn.metrics import classification_report
preds = bert_model.predict(train_x)

start_indexes = np.argmax(preds[0], axis=-1)
end_indexes = np.argmax(preds[1], axis=-1)

# start_index의 f1_score 85%
print(classification_report(train_y[0], start_indexes))

# end_index의 f1_score 88%
print(classification_report(train_y[1], end_indexes))

"""우리가 훈련한 BERT MODEL을 구글 GDrive에 저장합니다."""

# 87.7% 79.6%
# 모델 저장하기
path = "gdrive/My Drive/Colab Notebooks/squad"
bert_model.save_weights(path+"/(Uncased)Squad.h5")

#model.save_weights는 기본 bert 모델을 불러온 후 그 모델에 트레인 데이터로 훈련된 모델을 덮어 씌우는 것

# 저장한 bert_model 로드하기
bert_model = get_bert_finetuning_model(model)
path = "gdrive/My Drive/Colab Notebooks/squad"
bert_model.load_weights(path+"/(Uncased)Squad.h5")

"""우리가 만든 모델이 잘 작동하는지 확인해보도록 하겠습니다.  
모델을 훈련할 때 사용하지 않은 test data로 모델이 잘 작동하는지, 우리가 만든 버트 모델의 성능이 어느정도 되는지 알아보는 시간을 가져보겠습니다.  

SQAUD 데이터 셋에서 test 용도로 쓰이는 dev 파일을 PANDAS DATAFRAME 형식으로 불러오는 함수를 정의합니다.  
train 데이터와 모양이 약간 다르기 때문에, 함수를 새로 정의해야 합니다.
"""

def squad_json_to_dataframe_dev(input_file_path, record_path = ['data','paragraphs','qas','answers'],
                           verbose = 1):
    """
    input_file_path: path to the squad json file.
    record_path: path to deepest level in json file default value is
    ['data','paragraphs','qas','answers']
    verbose: 0 to suppress it default is 1
    """
    if verbose:
        print("Reading the json file")    
    file = json.loads(open(input_file_path).read())
    if verbose:
        print("processing...")
    # parsing different level's in the json file
    js = pd.io.json.json_normalize(file , record_path )
    m = pd.io.json.json_normalize(file, record_path[:-1] )
    r = pd.io.json.json_normalize(file,record_path[:-2])
    
    #combining it into single dataframe
    idx = np.repeat(r['context'].values, r.qas.str.len())
    m['context'] = idx
    main = m[['id','question','context','answers']].set_index('id').reset_index()
    main['c_id'] = main['context'].factorize()[0]
    if verbose:
        print("shape of the dataframe is {}".format(main.shape))
        print("Done")
    return main

input_file_path ='dev-v1.1.json'
record_path = ['data','paragraphs','qas','answers']
verbose = 0
dev = squad_json_to_dataframe_dev(input_file_path=input_file_path,record_path=record_path)

"""TEST DATA가 잘 불려왔는지 확인해 보겠습니다."""

dev

"""train 데이터와 다르게 SQuAD의 dev 데이터의 정답은 1개 이상입니다.  
따라서 여러 정답 중 하나 이상만 버트 모형이 맞춘다면, 정답을 맞춘 것으로 인정하는 것입니다.
"""

# 정답의 개수를 정의하는 칼럼 생성
dev['answer_len'] = dev['answers'].map(lambda x: len(x))

print(dev['answers'][0])
print()
print(dev['answers'][0][0]['text'])

# 정답들을 다루기 쉽게 리스트로 반환하는 함수 정의
def get_text(text_len, answers):
  # text_len : 질문(question)과 문장(context)에 해당하는 정답의 개수
  # answers : 정답 ex) [{'answer_start': 177, 'text': 'Denver Broncos'}, {'answer_start': 177, 'text': 'Denver Broncos'}, {'answer_start': 177, 'text': 'Denver Broncos'}]
  texts = []
  for i in range(text_len):
    texts.append(answers[i]['text'])
  return texts

# 정답의 개수(3)와 정답들([{'answer_start' : 177, 'text': 'Denver Broncos'}.....])을 인풋으로 받아서
# 정답들을 리스트로 바꿔주는 함수 실행
get_text(3, dev['answers'][0])

# texts 칼럼의 모든 데이터에 대해서 수행
dev['texts'] = dev.apply(lambda x: get_text(x['answer_len'], x['answers']), axis=1)

dev.head(5)

dev['texts']

"""테스트 데이터에 대해서 결과를 확인합니다.  
훈련에 사용하지 않은 테스트 데이터에 대한 예측을 제법 잘 수행하는 것을 보실 수 있겠습니다.

딥러닝에서 중요한 것 중 하나는 모델의 성능이 얼마나 되는지의 척도입니다.
**F1 Score**와 **Exact Match**를 측정해 보도록 하겠습니다.  
SQuAD에서의 **F1 Score**는 예측 단어들 중에서 얼마나 버트 모형이 예측한 것과 일치하는 지를 나타내는 척도입니다.  
예를 들면, 버트가 정답으로 [new, england, patriots]를 예측하였고, 실제 정답 또한 [new, england, patriots]라면, 예측 단어와 정답 단어가 모두 다 일치하기 때문에 F1 Score는 1.0 입니다.  
만약에, 버트가 정답으로 [england, patriots]만 예측했다면 F1 Score는 세 단어 중 2가지만 예측했기 때문에, 0.66(66%)가 됩니다.  
  
**EM**(Exact Match)은 정답단어들과 예측단어들이 완전히 일치하면 1, 일치하지 않으면 0입니다.  
정답, 예측값 모두 다 [new, england, patriots]가 되어야 합니다.  
  
**F1**과 **EM**을 계산해보도록 하겠습니다.
"""

TEXT_COLUMN = 'texts'

"""아까 TRAIN 데이터를 버트 인풋으로 변환하기 위한 함수를 살짝 변형하도록 하겠습니다.  
우리가 훈련한 버트 모형을 바탕으로 DEV 데이터를 예측하기 위해서 인풋을 만들어보도록 하겠습니다.  
TRAIN 데이터를 인풋으로 만들기 위한 함수랑 다른 점은, 정답을 출력하지 않는다는 점입니다. 예측만 하면 되니까요.
"""

def convert_data(data_df):
    global tokenizer
    indices, segments, target_start, target_end = [], [], [], []

    for i in tqdm(range(len(data_df))):
        que, _ = tokenizer.encode(data_df[QUESTION_COLUMN][i])
        doc, _ = tokenizer.encode(data_df[DATA_COLUMN][i])
        doc.pop(0)

        que_len = len(que)
        doc_len = len(doc)

        if que_len > 64:
          que = que[:63]
          que.append(102)
        
        if len(que+doc) > SEQ_LEN:
          while len(que+doc) != SEQ_LEN:
            doc.pop(-1)

          doc.pop(-1)
          doc.append(102)

        segment = [0]*len(que) + [1]*len(doc) + [0]*(SEQ_LEN-len(que)-len(doc))
        if len(que + doc) <= SEQ_LEN:
          while len(que+doc) != SEQ_LEN:
            doc.append(0)

        ids = que + doc

        texts = data_df[TEXT_COLUMN][i]
        for text_element in texts:
          text = tokenizer.encode(text_element)[0]

          text_slide_len = len(text[1:-1])
          for j in range(0,(len(doc))):  
              exist_flag = 0
              if text[1:-1] == doc[j:j+text_slide_len]:
                ans_start = j + len(que)
                ans_end = j + text_slide_len - 1 + len(que)
                exist_flag = 1
                break
        
          if exist_flag == 0:
            ans_start = SEQ_LEN
            ans_end = SEQ_LEN

        indices.append(ids)
        segments.append(segment)
        target_start.append(ans_start)
        target_end.append(ans_end)
        
    indices_x = np.array(indices)
    segments = np.array(segments)
    target_start = np.array(target_start)
    target_end = np.array(target_end)

    del_list = np.where(target_start!=SEQ_LEN)[0]
    not_del_list = np.where(target_start==SEQ_LEN)[0]
    indices_x = indices_x[del_list]
    segments = segments[del_list]

    target_start = target_start[del_list]
    target_end = target_end[del_list]
    
    return [indices_x, segments], del_list

def load_data(pandas_dataframe):
    data_df = pandas_dataframe
    data_df[DATA_COLUMN] = data_df[DATA_COLUMN].astype(str)
    data_df[QUESTION_COLUMN] = data_df[QUESTION_COLUMN].astype(str)
    data_x, data_y, del_list = convert_data(data_df)

    return data_x, data_y, del_list

dev_bert_input = convert_data(dev)

dev_bert_input

dev_bert_input, del_list = dev_bert_input[0], dev_bert_input[1]
dev = dev.iloc[del_list]
dev = dev.reset_index(drop=True)

dev

# 문장이 토큰화된 것 저장 (1~10527)
indexes = dev_bert_input[0]

indexes

# 버트 예측(384차원)
bert_predictions = bert_model.predict(dev_bert_input)

bert_predictions

# 예측값 중 가장 큰 값을 가진 부분 추출(가장 큰 확률이 위치해있는 곳)
start_indexes = np.argmax(bert_predictions[0], axis=-1)
end_indexes = np.argmax(bert_predictions[1], axis=-1)

# 정답 중 의미없는 부분 삭제
# 첫 번째 토큰에 해당하는 start_indexes가 마지막 토큰에 해당하는 end_indexes보다 작다면
# 말이 안 되므로, 이러한 경우를 제거
not_del_list = np.where(start_indexes <= end_indexes)[0]

not_del_list.shape

start_indexes = start_indexes[not_del_list]
end_indexes = end_indexes[not_del_list]
indexes = indexes[not_del_list]

start_indexes[0:100]

end_indexes[0:100]

# dev 데이터셋 재조정
dev = dev.iloc[not_del_list].reset_index(drop=True)

dev

# length : dev 데이터의 길이
length = len(dev)

sentences = []

untokenized = []

for j in range(len(start_indexes)):
  sentence = []
  for i in range(start_indexes[j], end_indexes[j]+1):
    token_based_word = reverse_token_dict[indexes[j][i]]
    sentence.append(token_based_word)
    # 문장이 토큰화된 단어 하나 하나를 sentence에 저장
  
  sentence_string = ""
  
  for w in sentence:
    
    if w.startswith("##"):
      w = w.replace("##", "")
      # 만약 sentence 안의 토큰이 ##으로 시작한다면, ##을 제거
    else:
      w = " " + w
      # 토큰이 ##으로 시작하지 않는다면 글자의 첫 시작이므로, 띄어쓰기 추가
    sentence_string += w
      # 리스트로 되어 있는 토큰들을 하나로 합쳐줌
  if sentence_string.startswith(" "):
    sentence_string = "" + sentence_string[1:]
    # sentence_string이 " "로 시작하는 경우에는 띄어쓰기를 삭제
  untokenized.append(sentence_string)
  # 리스트로 되어있는 토큰들을 하나로 합쳐준 것, 이것을 untokenized에 저장
  sentences.append(sentence)

print(sentences[:30])
print(untokenized[:30])

dev_answers = []
for i in range(length):
  dev_answer = []
  texts_dict = dev['answers'][i]
  
  for j in range(len(texts_dict)):
    dev_answer.append(texts_dict[j]['text'])
    # 정답 하나 하나를 리스트로 저장
  dev_answers.append(dev_answer)

dev_answers[0:10]

dev_tokens = []
for i in dev_answers:
  dev_tokened = []
  for j in i:
    temp_token = tokenizer.tokenize(j)
    # 정답을 토큰화
    temp_token.pop(0)
    # [CLS] 제거
    temp_token.pop(-1)
    # [SEP] 제거
    dev_tokened.append(temp_token)
  dev_tokens.append(dev_tokened)

print(dev_tokens[:5])

# 토큰화된 정답을 문장으로 변환시켜주고 합쳐줌
dev_answer_lists = []
for dev_answers in dev_tokens:
  dev_answer_list = []
  for dev_answer in dev_answers:
    dev_answer_string = " ".join(dev_answer)
    dev_answer_list.append(dev_answer_string)
  dev_answer_lists.append(dev_answer_list)

print(dev_answer_lists[:5])

# untokenizing
dev_strings_end = []
for dev_strings in dev_answer_lists:
  dev_strings_processed = []
  for dev_string in dev_strings:
    dev_string = dev_string.replace(" ##", "")
    dev_strings_processed.append(dev_string)
  dev_strings_end.append(dev_strings_processed)

dev_answers = dev_strings_end

print(dev_answers[:5])

"""F1 SCORE와 EXACT MATCH를 계산하는 함수 정의  
출처 : https://github.com/allenai/bi-att-flow/blob/master/squad/evaluate-v1.1.py
"""

from collections import Counter
import string

def normalize_answer(s):
    """Lower text and remove punctuation, articles and extra whitespace."""
    def remove_articles(text):
        return re.sub(r'\b(a|an|the)\b', ' ', text)

    def white_space_fix(text):
        return ' '.join(text.split())

    def remove_punc(text):
        exclude = set(string.punctuation)
        return ''.join(ch for ch in text if ch not in exclude)

    def lower(text):
        return text.lower()

    return white_space_fix(remove_articles(remove_punc(lower(s))))

def f1_score(prediction, ground_truth):
    prediction_tokens = normalize_answer(prediction).split()
    ground_truth_tokens = normalize_answer(ground_truth).split()
    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)
    num_same = sum(common.values())
    if num_same == 0:
        return 0
    precision = 1.0 * num_same / len(prediction_tokens)
    recall = 1.0 * num_same / len(ground_truth_tokens)
    f1 = (2 * precision * recall) / (precision + recall)
    return f1

def exact_match_score(prediction, ground_truth):
    return (normalize_answer(prediction) == normalize_answer(ground_truth))

def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):
    scores_for_ground_truths = []
    for ground_truth in ground_truths:
        score = metric_fn(prediction, ground_truth)
        scores_for_ground_truths.append(score)
    return max(scores_for_ground_truths)

f1_sum = 0

for i in range(len(untokenized)):
  f1 = metric_max_over_ground_truths(f1_score, untokenized[i], dev_answers[i])
  f1_sum += f1
print("f1 score : ", f1_sum / length)

EM_sum = 0

for i in range(len(untokenized)):
  
  EM = metric_max_over_ground_truths(exact_match_score, untokenized[i], dev_answers[i])
  EM_sum += EM
print("EM Score : ", EM_sum / length)

def convert_data(data_df):
    global tokenizer
    indices, segments = [], []

  
    que, _ = tokenizer.encode(data_df[QUESTION_COLUMN])
    doc, _ = tokenizer.encode(data_df[DATA_COLUMN])
    doc.pop(0)

    que_len = len(que)
    doc_len = len(doc)

    if que_len > 64:
      que = que[:63]
      que.append(102)
        
    if len(que+doc) > SEQ_LEN:
      while len(que+doc) != SEQ_LEN:
        doc.pop(-1)

      doc.pop(-1)
      doc.append(102)

    segment = [0]*len(que) + [1]*len(doc) + [0]*(SEQ_LEN-len(que)-len(doc))
    if len(que + doc) <= SEQ_LEN:
      while len(que+doc) != SEQ_LEN:
        doc.append(0)

    ids = que + doc


    indices.append(ids)
    segments.append(segment)
    indices = np.array(indices)
    segments = np.array(segments)
    return [indices, segments]

def predict_letter(df):
  
  test_input = convert_data(df)
  test_start, test_end = bert_model.predict(test_input)
  
  indexes = test_input[0].tolist()[0]
  start = np.argmax(test_start, axis=1).item()
  end = np.argmax(test_end, axis=1).item()
  start_tok = indexes[start]
  end_tok = indexes[end]

  print("Question : ", df['question'])
  print("-"*50)
  print("Context : ", end = " ")
  
  def split_text(text, n):
    for line in text.splitlines():
        while len(line) > n:
           x, line = line[:n], line[n:]
           yield x
        yield line
  
  for line in split_text(df['context'], 150):
    print(line)

  print("-"*50)
  print("ANSWER : ", end = " ")
  print("\n")
  sentences = []
  
  for i in range(start, end+1):
    token_based_word = reverse_token_dict[indexes[i]]
    sentences.append(token_based_word)
    print(token_based_word, end= " ")
  
  print("\n")
  print("Untokenized Answer : ", end = "")
  for w in sentences:
    if w.startswith("##"):
      w = w.replace("##", "")
    else:
      w = " " + w
    
    print(w, end="")
  print("")

"""임의의 Question과 Context를 받아서, 정답을 맞춰보는 것을 실험해보도록 하겠습니다.  

"""

import random
for i in random.sample(range(300),300):
  answers = dev['answers'][i]
  predict_letter(dev.iloc[i])
  print("")
  print("real answer : ", answers)
  print("")

